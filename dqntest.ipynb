{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLSwitch import RLSwitchEnv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from dqn import DQN\n",
    "from maxWeight import maxWeight\n",
    "from ppo import PPO\n",
    "# DQN Implementation\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "n = 3 # nxn\n",
    "end_t = 1000\n",
    "env = RLSwitchEnv(n,end_t, lambdaMatrix = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN setup\n",
    "dqn_agent = DQN(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344dec8baadb4feb985551c1e3b983bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trials  = 15\n",
    "steps = []\n",
    "for trial in tqdm(range(trials)):\n",
    "    env.reset() # Reset between each trial\n",
    "    cur_state = env.state # Get the current state\n",
    "#     for step in tqdm(range(end_t)): # Till the particular run ends\n",
    "    for step in range(end_t): # Till the particular run ends\n",
    "        action = dqn_agent.act(cur_state) # Get the action\n",
    "#         env.render()\n",
    "        new_state, reward, done, _ = env.step(action) # Get the rewards\n",
    "#         print(reward)\n",
    "        dqn_agent.remember(cur_state, action, \n",
    "            reward, new_state, done) # Store the states\n",
    "        dqn_agent.replay() # Learng\n",
    "        dqn_agent.target_train() # train target\n",
    "        cur_state = new_state # set new state\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.save_model(\"models/1stdqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "n = 3 # nxn\n",
    "end_t = 1000 # number of time steps to end an episode after\n",
    "env = RLSwitchEnv(n,end_t, lambdaMatrix = None)\n",
    "# test settings\n",
    "\n",
    "tests = 10\n",
    "rewardsDQN = np.zeros((tests,end_t))\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,tests)):\n",
    "    env.reset()\n",
    "    for j in range(0,end_t):\n",
    "        action = dqn_agent.act(env.state)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewardsDQN[i][j] = reward\n",
    "        if done:\n",
    "            env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Environment\n",
    "n = 3 # nxn\n",
    "end_t = 1000 # number of time steps to end an episode after\n",
    "env = RLSwitchEnv(n,end_t, lambdaMatrix = None)\n",
    "# test settings\n",
    "\n",
    "tests = 10\n",
    "rewardsRandom = np.zeros((tests,end_t))\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,tests)):\n",
    "    env.reset()\n",
    "    for j in range(0,end_t):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewardsRandom[i][j] = reward\n",
    "        if done:\n",
    "            env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxWeight Environment\n",
    "n = 3 # nxn\n",
    "end_t = 1000 # number of time steps to end an episode after\n",
    "env = RLSwitchEnv(n,end_t, lambdaMatrix = None)\n",
    "# test settings\n",
    "\n",
    "tests = 10\n",
    "rewardsMaxWeight = np.zeros((tests,end_t))\n",
    "\n",
    "maxWeightAgent = maxWeight(env)\n",
    "\n",
    "for i in tqdm(range(0,tests)):\n",
    "    env.reset()\n",
    "    for j in range(0,end_t):\n",
    "        action = maxWeightAgent.act()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewardsMaxWeight[i][j] = reward\n",
    "        if done:\n",
    "            env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO\n",
    "n = 3 # nxn\n",
    "end_t = 1000 # number of time steps to end an episode after\n",
    "env = RLSwitchEnv(n,end_t, lambdaMatrix = None)\n",
    "ppo_agent = PPO(env=env)\n",
    "state = env.reset()\n",
    "\n",
    "# setup\n",
    "state_dims = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "dummy_n = np.zeros((1, 1, n_actions))\n",
    "dummy_1 = np.zeros((1, 1, 1))\n",
    "target_reached = False\n",
    "\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while not target_reached and iters < max_iters:\n",
    "    states = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    masks = []\n",
    "    rewards = []\n",
    "    actions_probs = []\n",
    "    actions_onehot = []\n",
    "\n",
    "    for itr in range(ppo_steps):\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_dist = ppo_agent.actor.predict([state_input], steps=1)\n",
    "        q_value = ppo_agent.critic.predict([state_input], steps=1)\n",
    "        action = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "        action_onehot = np.zeros(n_actions)\n",
    "        action_onehot[action] = 1\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "                print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value))\n",
    "        mask = not done\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        actions_onehot.append(action_onehot)\n",
    "        values.append(q_value)\n",
    "        masks.append(mask)\n",
    "        rewards.append(reward)\n",
    "        actions_probs.append(action_dist)\n",
    "\n",
    "        state = observation\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "    \n",
    "    q_value = model_critic.predict(state_input, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRandom = np.average(rewardsRandom,axis=1)\n",
    "avgDQN = np.average(rewardsDQN,axis=1)\n",
    "avgmaxWeight = np.average(rewardsMaxWeight,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avgDQN)\n",
    "plt.plot(avgmaxWeight)\n",
    "plt.plot(avgRandom)\n",
    "plt.legend([\"DQN\",'maxWeight','Random'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RLSwitching] *",
   "language": "python",
   "name": "conda-env-RLSwitching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
